./venv/bin/python3 -m llama_cpp.server --model ./models/wizardlm-13b-v1.2.ggmlv3.q4_0.bin --n_gpu_layers 0 --n_ctx 8000 --use_mlock False
